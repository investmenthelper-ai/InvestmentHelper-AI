{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb5cda0-cda7-4d9f-abbe-20e37b19240d",
   "metadata": {},
   "source": [
    "## URL for KCHOL financial data on KAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929e42b-07dc-4a72-8fee-86d1c9ce94cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:13:12.910346Z",
     "start_time": "2024-12-22T17:12:56.358541Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException, HTTPError, ConnectionError, TooManyRedirects\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.kap.org.tr\"\n",
    "\n",
    "def download_document(firm_name, firm_id):\n",
    "    url = f\"https://www.kap.org.tr/tr/sirket-finansal-bilgileri/{firm_id}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Attempting to fetch main page for {firm_name}\")\n",
    "            response = session.get(url, headers=headers, allow_redirects=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            print(f\"Successfully fetched main page for {firm_name}\")\n",
    "            print(f\"Response URL: {response.url}\")\n",
    "            print(f\"Response status code: {response.status_code}\")\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the download link for the Excel document\n",
    "            download_link = soup.find('a', {'class': 'modal-button _3 type-small'})\n",
    "            if download_link and 'href' in download_link.attrs:\n",
    "                doc_url = urljoin(BASE_URL, download_link['href'])\n",
    "                \n",
    "                print(f\"Attempting to download document from: {doc_url}\")\n",
    "                \n",
    "                # Download the document\n",
    "                doc_response = session.get(doc_url, headers=headers, allow_redirects=True, timeout=30)\n",
    "                doc_response.raise_for_status()\n",
    "                \n",
    "                print(f\"Document downloaded. Content-Type: {doc_response.headers.get('Content-Type')}\")\n",
    "                \n",
    "                # Save the content\n",
    "                file_name = f\"{firm_name}_financial_report.xlsx\"\n",
    "                with open(file_name, 'wb') as file:\n",
    "                    file.write(doc_response.content)\n",
    "                print(f\"Document saved as {file_name}\")\n",
    "            else:\n",
    "                print(f\"Download link not found for {firm_name}.\")\n",
    "\n",
    "        except ConnectionError as e:\n",
    "            print(f\"Connection error occurred for {firm_name}: {e}\")\n",
    "            print(f\"URL attempted: {url}\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTP error occurred for {firm_name}: {e}\")\n",
    "            print(f\"URL attempted: {url}\")\n",
    "        except TooManyRedirects as e:\n",
    "            print(f\"Too many redirects for {firm_name}: {e}\")\n",
    "            print(f\"URL attempted: {url}\")\n",
    "        except RequestException as e:\n",
    "            print(f\"Error occurred while processing {firm_name}: {e}\")\n",
    "            print(f\"URL attempted: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred for {firm_name}: {e}\")\n",
    "            print(f\"URL attempted: {url}\")\n",
    "\n",
    "# Dictionary of firms with their correct identifiers\n",
    "firms = {\n",
    "    \"KCHOL\": \"4028e4a140f2ed710140f2f4d6c70039\",\n",
    "    \"TCELL\": \"4028e4a1486ec80a0148c55510d71d31\",\n",
    "    \"AKBNK\": \"4028e4a240e8d1830140e905edcd0006\"\n",
    "}\n",
    "\n",
    "for firm, firm_id in firms.items():\n",
    "    download_document(firm, firm_id)\n",
    "    time.sleep(5)  # Delay to be respectful to the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3d9f9-452a-4c57-9347-05c416112a6f",
   "metadata": {},
   "source": [
    "## Test to Translate File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0716935a-e649-484f-ba67-349f69645eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:38:03.851885Z",
     "start_time": "2024-12-22T17:38:03.735283Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Set your OpenAI API key\n",
    "\n",
    "def read_html_from_excel(file_path):\n",
    "    \"\"\"\n",
    "    Read HTML content from Excel file, handling both direct Excel reading\n",
    "    and text-based fallback if needed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        html_content = df.iloc[0, 0]\n",
    "        return html_content\n",
    "    except Exception as excel_error:\n",
    "        print(f\"Warning: Could not read as regular Excel file: {str(excel_error)}\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                if '<html' in content.lower() or '<body' in content.lower():\n",
    "                    return content\n",
    "                else:\n",
    "                    df = pd.read_excel(file_path, engine='openpyxl', dtype=str)\n",
    "                    return df.iloc[0, 0]\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Could not read file content: {str(e)}\")\n",
    "\n",
    "def is_translatable_content(text):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return False\n",
    "    number_pattern = r'^[\\d\\s,.%$€£¥+-/=()<>[\\]{}|#@!&_\\'\"]$'\n",
    "    if re.match(number_pattern, text):\n",
    "        return False\n",
    "    date_pattern = r'^[\\d\\s\\-./:|]*$'\n",
    "    if re.match(date_pattern, text):\n",
    "        return False\n",
    "    if not re.search(r'[a-zA-Z]', text):\n",
    "        return False\n",
    "    if len(text) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def query_openai(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.12\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error querying OpenAI: {str(e)}\")\n",
    "\n",
    "def translate_html_content(html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        excluded_tags = {'script', 'style', 'noscript', 'code', 'pre', 'time', 'meta'}\n",
    "\n",
    "        def should_translate(element):\n",
    "            if not isinstance(element, NavigableString):\n",
    "                return False\n",
    "            if element.parent.name in excluded_tags:\n",
    "                return False\n",
    "            if element.parent.has_attr('translate') and element.parent['translate'].lower() == 'no':\n",
    "                return False\n",
    "            return is_translatable_content(element.strip())\n",
    "\n",
    "        stats = {\n",
    "            'total_elements': 0,\n",
    "            'translated': 0,\n",
    "            'skipped_numbers': 0,\n",
    "            'skipped_empty': 0,\n",
    "            'failed': 0\n",
    "        }\n",
    "\n",
    "        for element in soup.find_all(string=True):\n",
    "            stats['total_elements'] += 1\n",
    "            text = element.strip()\n",
    "\n",
    "            if not text:\n",
    "                stats['skipped_empty'] += 1\n",
    "                continue\n",
    "\n",
    "            if not is_translatable_content(text):\n",
    "                stats['skipped_numbers'] += 1\n",
    "                continue\n",
    "\n",
    "            if should_translate(element):\n",
    "                try:\n",
    "                    prompt = f\"\"\"If the following text contains human-readable content, translate it into English and provide only the translation.\n",
    "If not, output nothing without any explanations or additional details.\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "\n",
    "                    translated_text = query_openai(prompt)\n",
    "                    element.replace_with(translated_text)\n",
    "                    stats['translated'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Translation failed for '{text[:50]}...': {str(e)}\")\n",
    "                    stats['failed'] += 1\n",
    "                    continue\n",
    "\n",
    "        print(\"\\nTranslation Statistics:\")\n",
    "        print(f\"Total elements processed: {stats['total_elements']}\")\n",
    "        print(f\"Successfully translated: {stats['translated']}\")\n",
    "        print(f\"Skipped numbers/symbols: {stats['skipped_numbers']}\")\n",
    "        print(f\"Skipped empty elements: {stats['skipped_empty']}\")\n",
    "        print(f\"Failed translations: {stats['failed']}\")\n",
    "\n",
    "        return str(soup)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error translating HTML: {str(e)}\")\n",
    "\n",
    "def save_html_to_file(html_content, output_file):\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error saving file: {str(e)}\")\n",
    "\n",
    "def process_excel_html(input_file, output_file):\n",
    "    try:\n",
    "        print(f\"Reading HTML from Excel file: {input_file}\")\n",
    "        html_content = read_html_from_excel(input_file)\n",
    "\n",
    "        if not html_content:\n",
    "            raise ValueError(\"No HTML content found in the Excel file\")\n",
    "\n",
    "        print(\"Validating HTML content...\")\n",
    "        if '<html' not in html_content.lower() and '<body' not in html_content.lower():\n",
    "            print(\"Warning: Content might not be proper HTML. Attempting to process anyway...\")\n",
    "\n",
    "        print(\"Translating content...\")\n",
    "        translated_html = translate_html_content(html_content)\n",
    "\n",
    "        print(f\"Saving translated content to: {output_file}\")\n",
    "        save_html_to_file(translated_html, output_file)\n",
    "\n",
    "        print(\"Processing completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "640daf3f-13a0-41e8-843c-58b63ab893ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:38:05.947443Z",
     "start_time": "2024-12-22T17:38:05.934455Z"
    }
   },
   "outputs": [],
   "source": [
    "input_file = 'TCELL_financial_report.xlsx'\n",
    "output_file = 'TCELL_financial_report_translated.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74dd69-17dd-4472-a8e4-ddfa159dbced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:38:49.771590Z",
     "start_time": "2024-12-22T17:38:10.418709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your actual file paths\n",
    "process_excel_html(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcad836f1499e4",
   "metadata": {},
   "source": [
    "## Formatting the HTML to give to a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dcb0c9c4ca821f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:40:06.718828Z",
     "start_time": "2024-12-22T17:40:06.681305Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Union, List, Dict\n",
    "\n",
    "def extract_content_after_header(soup, header_tags=('h1', 'h2', 'h3', 'h4', 'h5', 'h6')) -> Dict[str, Union[str, List[Dict[str, str]]]]:\n",
    "    \"\"\"\n",
    "    Extracts content after each header tag (h1-h6) and structures it in a dictionary.\n",
    "    Prioritizes tables over text within the same section.\n",
    "    \"\"\"\n",
    "    extracted_info = {}\n",
    "    for header in soup.find_all(header_tags):\n",
    "        section_title = header.get_text(strip=True)\n",
    "        print(f\"Detected header: {section_title}\")  # Debug print\n",
    "\n",
    "        content_block = None  # Stores either table or text content\n",
    "        next_elem = header.find_next()\n",
    "\n",
    "        while next_elem:\n",
    "            if next_elem.name in header_tags:\n",
    "                break  # Stop if the next header is encountered\n",
    "            if next_elem.name == 'table':\n",
    "                # Detect single-cell tables without headers\n",
    "                rows = next_elem.find_all(\"tr\")\n",
    "                if len(rows) == 1 and len(rows[0].find_all(\"td\")) == 1:\n",
    "                    # Single-cell table detected; treat as text content\n",
    "                    single_cell_text = rows[0].find(\"td\").get_text(strip=True)\n",
    "                    print(f\"Detected single-cell table under header '{section_title}' with text: {single_cell_text}\")\n",
    "                    content_block = {\"type\": \"text\", \"data\": single_cell_text}\n",
    "                    break  # Stop further processing for this section\n",
    "\n",
    "                # Otherwise, process as a multi-row table with headers\n",
    "                try:\n",
    "                    headers = [th.get_text(strip=True) for th in rows[0].find_all(\"td\")]\n",
    "                    table_data = [\n",
    "                        dict(zip(headers, [td.get_text(strip=True) for td in row.find_all(\"td\")]))\n",
    "                        for row in rows[1:]\n",
    "                        if row.find_all(\"td\")\n",
    "                    ]\n",
    "                    print(f\"Detected table under header '{section_title}' with data: {table_data}\")  # Debug print\n",
    "                    content_block = {\"type\": \"table\", \"data\": table_data}\n",
    "                    break  # Prioritize the table, ignore any subsequent text\n",
    "                except IndexError:\n",
    "                    print(f\"Table under header '{section_title}' is improperly formatted or empty.\")\n",
    "            elif (next_elem.name == 'p' or next_elem.name == 'div') and content_block is None:\n",
    "                # Only set text if no table has been detected\n",
    "                text_content = next_elem.get_text(strip=True)\n",
    "                if text_content:\n",
    "                    print(f\"Detected text under header '{section_title}': {text_content}\")  # Debug print\n",
    "                    content_block = {\"type\": \"text\", \"data\": text_content}\n",
    "\n",
    "            next_elem = next_elem.find_next()\n",
    "\n",
    "        if content_block:\n",
    "            extracted_info[section_title] = [content_block]\n",
    "        else:\n",
    "            print(f\"No content found under header '{section_title}'\")  # Debug print\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "def format_content_as_markdown(extracted_info) -> str:\n",
    "    \"\"\"\n",
    "    Formats the extracted information dictionary into Markdown format.\n",
    "    \"\"\"\n",
    "    formatted_output = \"# Extracted Content\\n\\n\"\n",
    "    for title, contents in extracted_info.items():\n",
    "        formatted_output += f\"## {title}\\n\\n\"\n",
    "        for content in contents:\n",
    "            if content[\"type\"] == \"table\" and content[\"data\"]:\n",
    "                headers = content[\"data\"][0].keys()\n",
    "                table_header = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "                separator = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "                table_rows = \"\".join([\"| \" + \" | \".join(row.values()) + \" |\\n\" for row in content[\"data\"]])\n",
    "                formatted_output += table_header + separator + table_rows + \"\\n\\n\"\n",
    "            elif content[\"type\"] == \"text\":\n",
    "                formatted_output += f\"{content['data']}\\n\\n\"\n",
    "\n",
    "    return formatted_output\n",
    "\n",
    "def extract_company_info_general(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Extract information in a generalized way\n",
    "    extracted_info = extract_content_after_header(soup)\n",
    "\n",
    "    # Convert extracted information to Markdown format\n",
    "    formatted_output = format_content_as_markdown(extracted_info)\n",
    "\n",
    "    # Save to a Markdown file\n",
    "    with open(\"extracted_content.md\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(formatted_output)\n",
    "\n",
    "    return formatted_output\n",
    "\n",
    "# Usage example\n",
    "file_path = \"TCELL_financial_report_translated.xlsx\"\n",
    "parsed_content = extract_company_info_general(file_path)\n",
    "print(parsed_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe5f1a25c00f64",
   "metadata": {},
   "source": [
    "# Chunking the related md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5953c1a29a093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T17:42:05.314429Z",
     "start_time": "2024-11-12T17:41:58.717188Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Load a model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_row_embeddings(rows: List[str]) -> List:\n",
    "    \"\"\"Generate embeddings for each row.\"\"\"\n",
    "    return model.encode(rows, convert_to_tensor=True)\n",
    "\n",
    "def create_chunks(rows: List[str], similarity_threshold: float = 0.7) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Automatically creates chunks of related rows based on semantic similarity.\n",
    "    Rows with similarity above the threshold are grouped together.\n",
    "    \"\"\"\n",
    "    embeddings = get_row_embeddings(rows)\n",
    "    chunks = []\n",
    "    current_chunk = [rows[0]]\n",
    "\n",
    "    for i in range(1, len(rows)):\n",
    "        similarity = util.pytorch_cos_sim(embeddings[i - 1], embeddings[i]).item()\n",
    "        \n",
    "        if similarity >= similarity_threshold:\n",
    "            current_chunk.append(rows[i])  # Add row to current chunk\n",
    "        else:\n",
    "            chunks.append(current_chunk)  # Finalize current chunk\n",
    "            current_chunk = [rows[i]]  # Start a new chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)  # Add last chunk\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_tables_by_similarity(table_data: Dict[str, List[str]], similarity_threshold: float = 0.7) -> Dict[str, List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Processes multiple tables by section, chunking related rows by similarity.\n",
    "    \"\"\"\n",
    "    all_chunks = {}\n",
    "    for section, rows in table_data.items():\n",
    "        section_chunks = create_chunks(rows, similarity_threshold)\n",
    "        all_chunks[section] = section_chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Example usage\n",
    "table_data = {\n",
    "    \"FİNANSAL DURUM TABLOSU\": [\n",
    "        \"| Sunum Para Birimi | 1000TL | 1000000TL | 1000000TL | 1000000TL |\",\n",
    "        \"| Finansal Tablo Niteliği | Konsolide | Konsolide | Konsolide | Konsolide |\",\n",
    "        \"| Dönen Varlıklar | 605.974.596 | 982.090 | 1.712.378 | 2.197.689 |\",\n",
    "        \"| Duran Varlıklar | 414.578.711 | 600.504 | 1.146.587 | 1.389.200 |\",\n",
    "        \"| Varlıklar | 1.020.553.307 | 1.582.594 | 2.858.965 | 3.586.889 |\",\n",
    "        \"| Kısa Vadeli Yükümlülükler | 709.676.931 | 1.145.655 | 1.865.813 | 2.403.087 |\",\n",
    "        \"| Uzun Vadeli Yükümlülükler | 189.771.314 | 189.741 | 318.880 | 388.901 |\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Process each section with similarity-based chunking\n",
    "all_chunks = chunk_tables_by_similarity(table_data, similarity_threshold=0.7)\n",
    "\n",
    "# Print the resulting chunks for each section\n",
    "for section, chunks in all_chunks.items():\n",
    "    print(f\"Section: {section}\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}:\\n\" + \"\\n\".join(chunk) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737594541e5a1a8f",
   "metadata": {},
   "source": [
    "# Result After Chunking (Mostly Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075c81da05ce5d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:47:11.157127Z",
     "start_time": "2024-12-22T17:43:20.322542Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "\n",
    "\n",
    "def read_markdown_file(file_path: str) -> str:\n",
    "    \"\"\"Reads the markdown file content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def extract_tables(content: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts tables from the markdown content.\n",
    "    Each table is separated by empty lines for easy processing.\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    lines = content.splitlines()\n",
    "    current_table = []\n",
    "    is_table = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"|\" in line:  # Detect a table line by presence of \"|\"\n",
    "            is_table = True\n",
    "            current_table.append(line)\n",
    "        elif is_table:\n",
    "            # End of a table section\n",
    "            tables.append(\"\\n\".join(current_table))\n",
    "            current_table = []\n",
    "            is_table = False\n",
    "\n",
    "    if current_table:\n",
    "        tables.append(\"\\n\".join(current_table))  # Add last table if any\n",
    "\n",
    "    return tables\n",
    "\n",
    "def query_openai(prompt: str) -> str:\n",
    "    \"\"\"Queries OpenAI with a given prompt and returns the response.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.12\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error querying OpenAI: {str(e)}\")\n",
    "\n",
    "def process_table_rows_with_header(header: str, rows: List[str], chunk_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Processes each table row in chunks with the given header.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    for i in range(0, len(rows), chunk_size):\n",
    "        chunk = rows[i:i + chunk_size]\n",
    "        chunk_text = \"\\n\".join(chunk)\n",
    "        prompt = (\n",
    "            f\"This table provides financial or operational data. The header indicates the structure, \"\n",
    "            f\"and the following rows contain detailed information. Analyze the rows based on this structure:\\n\\n\"\n",
    "            f\"Header: {header}\\n\\n\"\n",
    "            f\"Rows:\\n{chunk_text}\\n\\n\"\n",
    "            f\"Please provide a detailed analysis of the data, including specific numerical observations and relationships. \"\n",
    "            f\"Do not speculate or mention missing context.\"\n",
    "        )\n",
    "        response = query_openai(prompt)\n",
    "        responses.append(response)\n",
    "        print(f\"Processed chunk with header '{header}':\\n{response}\\n\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "def replace_tables_with_responses(content: str, tables_responses: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Replaces original tables in the content with the corresponding AI responses.\n",
    "    \"\"\"\n",
    "    lines = content.splitlines()\n",
    "    modified_content = []\n",
    "    table_index = 0\n",
    "    is_table = False\n",
    "    current_table = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"|\" in line:\n",
    "            is_table = True\n",
    "            current_table.append(line)\n",
    "        elif is_table:\n",
    "            # Replace table with AI response\n",
    "            if table_index < len(tables_responses):\n",
    "                modified_content.append(tables_responses[table_index])\n",
    "                table_index += 1\n",
    "            is_table = False\n",
    "            current_table = []\n",
    "        else:\n",
    "            if not is_table:\n",
    "                modified_content.append(line)\n",
    "\n",
    "    return \"\\n\".join(modified_content)\n",
    "\n",
    "def process_and_replace_tables(file_path: str, output_path: str, chunk_size: int):\n",
    "    \"\"\"Reads a markdown file, processes tables, and replaces them with AI-generated responses.\"\"\"\n",
    "    # Step 1: Read file content\n",
    "    content = read_markdown_file(file_path)\n",
    "    \n",
    "    # Step 2: Extract tables\n",
    "    tables = extract_tables(content)\n",
    "    \n",
    "    # Step 3: Process each table by sending the header and row data in chunks\n",
    "    tables_responses = []\n",
    "    for table in tables:\n",
    "        rows = table.splitlines()\n",
    "        header, *data_rows = rows\n",
    "        table_response = process_table_rows_with_header(header, data_rows, chunk_size)\n",
    "        tables_responses.append(\"\\n\".join(table_response))\n",
    "    \n",
    "    # Step 4: Replace original tables with AI responses\n",
    "    modified_content = replace_tables_with_responses(content, tables_responses)\n",
    "    \n",
    "    # Step 5: Save to a new markdown file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(modified_content)\n",
    "    print(f\"Modified content saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"extracted_content.md\"\n",
    "output_path = \"modified_content.md\"\n",
    "chunk_size = 3  # Set the desired number of rows per chunk\n",
    "process_and_replace_tables(file_path, output_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d01facb67d02b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
